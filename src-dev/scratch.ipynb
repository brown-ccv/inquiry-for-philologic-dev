{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n",
      "Not string\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import multiprocessing\n",
    "import enchant\n",
    "import config\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "def tag2pos(tag, returnNone=False):\n",
    "    ap_tag = {'NN': wn.NOUN, 'JJ': wn.ADJ,\n",
    "              'VB': wn.VERB, 'RB': wn.ADV}\n",
    "    try:\n",
    "        return ap_tag[tag[:2]]\n",
    "    except:\n",
    "        return None if returnNone else ''\n",
    "\n",
    "\n",
    "def lemmatize_df(df):\n",
    "    # lemmatize speech acts\n",
    "    for index, row in df.iterrows():\n",
    "        # initialize lemma list\n",
    "        lemma_list = []\n",
    "        # tokenize word_pos\n",
    "        if type(row['SPEECH_ACT']) == str:\n",
    "            tokens = word_tokenize(row['SPEECH_ACT'])\n",
    "            alpha_tokens = [token for token in tokens if token.isalpha()]\n",
    "            spellchecked_tokens = [token for token in alpha_tokens\n",
    "                                   if dictionary.check(token)]\n",
    "            tagged_tokens = pos_tag(spellchecked_tokens)\n",
    "            for tagged_token in tagged_tokens:\n",
    "                word = str(tagged_token[0])\n",
    "                word_pos = tagged_token[1]\n",
    "                word_pos_morphed = tag2pos(word_pos)\n",
    "                if word_pos_morphed is not '':\n",
    "                    lemma = lemmatizer.lemmatize(word, word_pos_morphed)\n",
    "                else:\n",
    "                    lemma = lemmatizer.lemmatize(word)\n",
    "                lemma_list.append(lemma)\n",
    "            lemma_string = ' '.join(lemma_list)\n",
    "            df.loc[index, 'LEMMAS'] = lemma_string\n",
    "        else:\n",
    "            print \"Not string\"\n",
    "            df.loc[index, 'SPEECH_ACT'] = 'not string'\n",
    "            df.loc[index, 'LEMMAS'] = 'not string'\n",
    "            continue\n",
    "\n",
    "    return(df)\n",
    "    # df.to_csv(\"/Users/alee35/land-wars-devel-data/03.lemmatized_speech_acts/membercontributions-lemmatized.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "# load British English spell checker\n",
    "dictionary = enchant.Dict(\"en_GB\")\n",
    "# lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# create as many processes as there are CPUs on your machine\n",
    "num_processes = multiprocessing.cpu_count()/2\n",
    "# calculate the chunk size as an integer\n",
    "chunk_size = int(config.text.shape[0]/num_processes)\n",
    "# works even if the df length is not evenly divisible by num_processes\n",
    "chunks = [config.text.ix[config.text.index[i:i + chunk_size]]\n",
    "          for i in range(0, config.text.shape[0], chunk_size)]\n",
    "# create our pool with `num_processes` processes\n",
    "pool = multiprocessing.Pool(processes=num_processes)\n",
    "# apply our function to each chunk in the list\n",
    "result = pool.map(lemmatize_df, chunks)\n",
    "# combine the results from our pool to a dataframe\n",
    "config.textlem = pd.DataFrame().reindex_like(config.text)\n",
    "config.textlem['LEMMAS'] = np.NaN\n",
    "for i in range(len(result)):\n",
    "    config.textlem.ix[result[i].index] = result[i]\n",
    "\n",
    "    \n",
    "config.textlem.to_csv(\"/Users/alee35/land-wars-devel-data/03.lemmatized_speech_acts/membercontributions-lemmatized.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "# write speech acts to files for triplet tagging\n",
    "for index, row in config.textlem.iterrows():\n",
    "    f = '/Users/alee35/Google Drive/repos/Stanford-OpenIE-Python/hansard/debate_{}.txt'.format(index)\n",
    "    with open(f, 'w') as f:\n",
    "        f.write(row['LEMMAS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config.textlem.to_csv(\"/Users/alee35/land-wars-devel-data/03.lemmatized_speech_acts/membercontributions-lemmatized.tsv\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
